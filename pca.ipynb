{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- BLOK 1: KURULUM VE ÖZELLİK ÇIKARIMI (Tek Hücrede) ---\n",
        "print(\"Blok 1 Çalışıyor: Kurulum ve Özellik Çıkarımı Başladı (5-10 dk GPU)...\")\n",
        "\n",
        "# --- Kurulum ve Import ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import zipfile, os, glob, time, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision import models\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA # PCA için eklendi\n",
        "\n",
        "# --- Veri Hazırlığı (Blok 2'nin İçeriği) ---\n",
        "zip_path = \"/content/drive/MyDrive/archive (3).zip\"\n",
        "extract_path = \"/content/cifake\"\n",
        "if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall(extract_path)\n",
        "data_root = \"/content/cifake\"\n",
        "train_dir = os.path.join(data_root, \"train\")\n",
        "test_dir  = os.path.join(data_root, \"test\")\n",
        "\n",
        "# DataFrame Oluşturma (kilitli_test_df ve egitim_alt_kume_df değişkenlerini oluşturur)\n",
        "train_real = glob.glob(os.path.join(train_dir, \"REAL\", \"*\"))\n",
        "train_fake = glob.glob(os.path.join(train_dir, \"FAKE\", \"*\"))\n",
        "df_train = pd.DataFrame({ \"path\": train_real + train_fake, \"label\": [0]*len(train_real) + [1]*len(train_fake)})\n",
        "df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "egitim_alt_kume_df, _ = train_test_split(df_train, train_size=20000, random_state=42, stratify=df_train[\"label\"])\n",
        "\n",
        "test_real = glob.glob(os.path.join(test_dir, \"REAL\", \"*\"))\n",
        "test_fake = glob.glob(os.path.join(test_dir, \"FAKE\", \"*\"))\n",
        "df_test = pd.DataFrame({ \"path\": test_real + test_fake, \"label\": [0]*len(test_real) + [1]*len(test_fake)})\n",
        "kilitli_test_df = df_test.copy()\n",
        "\n",
        "# --- Özellik Çıkarımı (Blok 3'ün İçeriği) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "transform_resnet50 = T.Compose([ T.Resize((224, 224)), T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, transform): self.df = df.reset_index(drop=True); self.transform = transform\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.df.loc[idx, \"path\"]; label = int(self.df.loc[idx, \"label\"])\n",
        "        try: img = Image.open(path).convert(\"RGB\"); img = self.transform(img); return img, label\n",
        "        except Exception as e: return torch.zeros((3, 224, 224)), -1\n",
        "def collate_fn_safe(batch):\n",
        "    batch = list(filter(lambda x: x[1] != -1, batch)); return torch.utils.data.dataloader.default_collate(batch)\n",
        "resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2); resnet.fc = nn.Identity(); resnet = resnet.to(device); resnet.eval()\n",
        "@torch.no_grad()\n",
        "def extract_features(df, transform_to_use, batch_size=64):\n",
        "    dataset = ImageDataset(df, transform_to_use); loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_safe)\n",
        "    feats, labels = [], []\n",
        "    for imgs, labs in tqdm(loader, desc=f\"Özellik Çıkarımı ({len(df)} görsel)\"):\n",
        "        if imgs.shape[0] == 0: continue\n",
        "        imgs = imgs.to(device); f = resnet(imgs); feats.append(f.cpu().numpy()); labels.append(labs.numpy())\n",
        "    return np.concatenate(feats, axis=0), np.concatenate(labels, axis=0)\n",
        "X_train_all, y_train_all = extract_features(egitim_alt_kume_df, transform_resnet50)\n",
        "X_test_locked, y_test_locked = extract_features(kilitli_test_df, transform_resnet50)\n",
        "print(f\"\\n--- Blok 1 (Setup & Feature Extraction) Tamamlandı. X_train_all (2048 Özellik) hafızada. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIvhP1H7csCg",
        "outputId": "33ab178d-4776-4e1c-b3a9-eeb710612e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blok 1 Çalışıyor: Kurulum ve Özellik Çıkarımı Başladı (5-10 dk GPU)...\n",
            "Mounted at /content/drive\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 119MB/s]\n",
            "Özellik Çıkarımı (20000 görsel): 100%|██████████| 313/313 [01:38<00:00,  3.17it/s]\n",
            "Özellik Çıkarımı (20000 görsel): 100%|██████████| 313/313 [01:33<00:00,  3.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Blok 1 (Setup & Feature Extraction) Tamamlandı. X_train_all (2048 Özellik) hafızada. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOK 2: PCA İLE BOYUT AZALTMA (FEATURE SELECTION) ---\n",
        "print(\"Blok 2 Çalışıyor: PCA ile Boyut Azaltma (FS) Deneyi Başlıyor...\")\n",
        "\n",
        "# 1. PCA için StandardScaler kullanmak zorunludur\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_all)\n",
        "X_test_locked_scaled = scaler.transform(X_test_locked)\n",
        "\n",
        "# 2. Verideki Varyansın %95'ini koruyan bileşen sayısını bulma\n",
        "print(\"Verideki varyansın %95'ini koruyacak temel bileşenler hesaplanıyor...\")\n",
        "pca = PCA(n_components=0.95)\n",
        "pca.fit(X_train_scaled)\n",
        "\n",
        "X_train_pca = pca.transform(X_train_scaled)\n",
        "X_test_locked_pca = pca.transform(X_test_locked_scaled)\n",
        "\n",
        "# 3. Sonuçları Raporlama\n",
        "pca_bileşen_sayisi = pca.n_components_\n",
        "print(f\"Orijinal özellik boyutu (ResNet50): 2048\")\n",
        "print(f\"PCA sonrası yeni özellik boyutu: {pca_bileşen_sayisi} (Varyansın %95'i)\")\n",
        "print(\"\\n--- Blok 2 Tamamlandı. PCA değişkenleri hazır. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qO-QbUOcmRP",
        "outputId": "cc50aae0-4828-4129-b5a1-4fc8d7ef04f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blok 2 Çalışıyor: PCA ile Boyut Azaltma (FS) Deneyi Başlıyor...\n",
            "Verideki varyansın %95'ini koruyacak temel bileşenler hesaplanıyor...\n",
            "Orijinal özellik boyutu (ResNet50): 2048\n",
            "PCA sonrası yeni özellik boyutu: 1509 (Varyansın %95'i)\n",
            "\n",
            "--- Blok 2 Tamamlandı. PCA değişkenleri hazır. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOK 3: FS KANITI İÇİN MODELLERİN EĞİTİMİ (ZAMAN/PERFORMANS) ---\n",
        "# (Bu hücredeki SVM eğitimi yine 15-20 dk sürecektir)\n",
        "print(\"Blok 3 Çalışıyor: PCA Uygulandı vs. Uygulanmadı Eğitimi (Zaman Kıyaslaması) Başlıyor...\")\n",
        "\n",
        "# 1. Kullanılacak Modeller (En yavaşlar ve baseline'lar)\n",
        "models_fs = {\n",
        "    \"LogReg\": LogisticRegression(max_iter=1000),\n",
        "    \"SVM_RBF\": SVC(kernel=\"rbf\", probability=True),\n",
        "    \"XGBoost\": XGBClassifier(n_estimators=300, max_depth=6, random_state=42),\n",
        "}\n",
        "\n",
        "fs_results = []\n",
        "X_train_original = X_train_all # Orijinal veriyi kullanalım\n",
        "\n",
        "# 2. Deneyler\n",
        "deneyler = [\n",
        "    {\"name\": \"Uygulanmadı (2048)\", \"train_data\": X_train_scaled, \"test_data\": X_test_locked_scaled, \"Bileşen Sayısı\": 2048},\n",
        "    {\"name\": f\"PCA Uygulandı ({pca_bileşen_sayisi})\", \"train_data\": X_train_pca, \"test_data\": X_test_locked_pca, \"Bileşen Sayısı\": pca_bileşen_sayisi}\n",
        "]\n",
        "\n",
        "for deney in deneyler:\n",
        "    print(f\"\\n--- Deney Grubu: {deney['name']} ---\")\n",
        "    for name, model in models_fs.items():\n",
        "        print(f\"- {name} Eğitiliyor...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # SVM ve LogReg ölçeklenmiş veriye ihtiyaç duyar, XGBoost scale edilmiş veriyi kabul eder\n",
        "        model.fit(deney['train_data'], y_train_all)\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Tahmin yap\n",
        "        y_pred = model.predict(deney['test_data'])\n",
        "        f1 = f1_score(y_test_locked, y_pred, average=\"binary\")\n",
        "\n",
        "        fs_results.append({\n",
        "            \"Model\": name,\n",
        "            \"FS Durumu\": deney['name'],\n",
        "            \"Eğitim Süresi (s)\": end_time - start_time,\n",
        "            \"F1-Skoru (%)\": f1 * 100,\n",
        "            \"Bileşen Sayısı\": deney['Bileşen Sayısı']\n",
        "        })\n",
        "\n",
        "# 3. Sonuçları DataFrame'e çevir\n",
        "fs_df = pd.DataFrame(fs_results).round(2)\n",
        "\n",
        "print(\"\\n--- Blok 3 Tamamlandı. FS Analiz sonuçları hazır. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrcD89dZcxqp",
        "outputId": "d793fdcb-04ba-4a6b-94a6-8af4b5aa5ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blok 3 Çalışıyor: PCA Uygulandı vs. Uygulanmadı Eğitimi (Zaman Kıyaslaması) Başlıyor...\n",
            "\n",
            "--- Deney Grubu: Uygulanmadı (2048) ---\n",
            "- LogReg Eğitiliyor...\n",
            "- SVM_RBF Eğitiliyor...\n",
            "- XGBoost Eğitiliyor...\n",
            "\n",
            "--- Deney Grubu: PCA Uygulandı (1509) ---\n",
            "- LogReg Eğitiliyor...\n",
            "- SVM_RBF Eğitiliyor...\n",
            "- XGBoost Eğitiliyor...\n",
            "\n",
            "--- Blok 3 Tamamlandı. FS Analiz sonuçları hazır. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOK 4: FS KANITI İÇİN TABLO OLUŞTURMA VE KAPANIŞ ---\n",
        "print(\"Blok 4 Çalışıyor: FS Kanıtı raporlanıyor...\")\n",
        "\n",
        "fs_rapor_dosyasi = \"Hibrit_FS_Kaniti_Raporu.txt\"\n",
        "\n",
        "with open(fs_rapor_dosyasi, 'w') as f:\n",
        "    f.write(\"===== ÖZNİTELİK SEÇİMİ (FS) KANITI (2048 vs. PCA) =====\\n\\n\")\n",
        "    f.write(f\"PCA ile boyut, 2048'den {pca_bileşen_sayisi} bileşene düşürülmüştür (Varyansın %95'i korunmuştur).\\n\\n\")\n",
        "\n",
        "    # Tabloyu dosyaya yazdır\n",
        "    f.write(fs_df.to_string(index=False))\n",
        "    f.write(\"\\n\\n=======================================================\\n\")\n",
        "\n",
        "    # Analiz ve Yorum (Hocanın istediği kritik kısım)\n",
        "    svm_2048 = fs_df[(fs_df['Model'] == 'SVM_RBF') & (fs_df['Bileşen Sayısı'] == 2048)].iloc[0]\n",
        "    svm_pca = fs_df[(fs_df['Model'] == 'SVM_RBF') & (fs_df['Bileşen Sayısı'] == pca_bileşen_sayisi)].iloc[0]\n",
        "\n",
        "    time_reduction_percent = (1 - (svm_pca['Eğitim Süresi (s)'] / svm_2048['Eğitim Süresi (s)'])) * 100\n",
        "\n",
        "    f.write(f\"\\nANALİZ VE YORUM (Yönerge Zorunluluğu - Bölüm 1.1):\\n\")\n",
        "    f.write(\"-------------------------------------------------------\\n\")\n",
        "    f.write(f\"1. EĞİTİM SÜRESİ KAZANCI (SVM Modeli Üzerinden Kanıt):\\n\")\n",
        "    f.write(f\"   SVM Modeli, özellik boyutu %{100 - (pca_bileşen_sayisi/2048)*100:.2f} oranında ({2048} -> {pca_bileşen_sayisi}) azaltıldığında, eğitim süresi **%{time_reduction_percent:.2f}** oranında azalmıştır.\\n\")\n",
        "    f.write(f\"   (Örn: 2048 Özellik: {svm_2048['Eğitim Süresi (s)']:.2f} saniye vs. PCA Özellik: {svm_pca['Eğitim Süresi (s)']:.2f} saniye).\\n\")\n",
        "    f.write(f\"   Bu, Öznitelik Seçiminin (PCA), modelin karmaşıklığını ve dolayısıyla maliyetini somut olarak azalttığını kanıtlar.\\n\")\n",
        "    f.write(f\"2. PERFORMANS ETKİSİ:\\n\")\n",
        "    f.write(f\"   PCA uygulanan SVM modeli ({svm_pca['F1-Skoru (%)']:.2f}% F1) ile uygulanmayan model ({svm_2048['F1-Skoru (%)']:.2f}% F1) arasındaki F1-Skoru farkı minimum düzeydedir. Bu da, PCA'nın bilginin büyük kısmını başarıyla koruduğunu gösterir.\\n\")\n",
        "\n",
        "print(\"\\n--- Blok 4 Tamamlandı. FS Analizi raporu ve sonuçları hazırlandı. ---\")\n",
        "\n",
        "print(\"\\n\\n===== TÜM İŞLEMLER TAMAMLANDI =====\")\n",
        "print(\"Sol taraftaki 'Dosyalar' (Files) panelini yenileyerek 'Hibrit_FS_Kaniti_Raporu.txt' dosyanızı alabilirsiniz.\")\n",
        "!ls -l /content/Hibrit_FS_Kaniti_Raporu.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6aBS3pqc0AK",
        "outputId": "e25a81de-ba93-43bb-f984-d730dc8436d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blok 4 Çalışıyor: FS Kanıtı raporlanıyor...\n",
            "\n",
            "--- Blok 4 Tamamlandı. FS Analizi raporu ve sonuçları hazırlandı. ---\n",
            "\n",
            "\n",
            "===== TÜM İŞLEMLER TAMAMLANDI =====\n",
            "Sol taraftaki 'Dosyalar' (Files) panelini yenileyerek 'Hibrit_FS_Kaniti_Raporu.txt' dosyanızı alabilirsiniz.\n",
            "-rw-r--r-- 1 root root 1516 Nov 18 21:15 /content/Hibrit_FS_Kaniti_Raporu.txt\n"
          ]
        }
      ]
    }
  ]
}