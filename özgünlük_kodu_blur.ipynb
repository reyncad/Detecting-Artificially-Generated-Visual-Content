{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- BLOK 1: KURULUM, DRIVE VE KÜTÜPHANELER ---\n",
        "print(\"Blok 1 Çalışıyor: Kurulum, Drive Bağlantısı ve Kütüphaneler...\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile, os\n",
        "import time\n",
        "import json\n",
        "import io # JPEG sıkıştırması için eklendi\n",
        "\n",
        "# 1. ZIP dosyasını açma (Eğer 'cifake' klasörü yoksa)\n",
        "zip_path = \"/content/drive/MyDrive/archive (3).zip\"\n",
        "extract_path = \"/content/cifake\"\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    print(f\"'{extract_path}' bulunamadı, ZIP açılıyor (5-10 dk sürebilir)...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall(extract_path)\n",
        "else:\n",
        "    print(f\"'{extract_path}' zaten mevcut, ZIP açma adımı atlanıyor.\")\n",
        "\n",
        "# 2. Gerekli kütüphaneyi kurma\n",
        "!pip install xgboost\n",
        "\n",
        "# 3. Gerekli tüm kütüphaneleri import etme\n",
        "import glob, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision import models\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
        "    confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") # Gereksiz uyarıları kapat\n",
        "\n",
        "print(\"\\n--- Blok 1 Tamamlandı. Kütüphaneler yüklendi. ---\")"
      ],
      "metadata": {
        "id": "5rxudk_Mg1AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOK 2: VERİ YOLU BULMA VE DATAFRAME OLUŞTURMA ---\n",
        "print(\"Blok 2 Çalışıyor: Veri Yolları Bulunuyor ve DataFrame'ler Oluşturuluyor...\")\n",
        "\n",
        "# 1. Veri setinin ana klasörünü bulma (içinde 'train' ve 'test' olan)\n",
        "search_root = \"/content/cifake\"\n",
        "data_root = None\n",
        "for dirpath, dirnames, filenames in os.walk(search_root):\n",
        "    if \"train\" in dirnames and \"test\" in dirnames:\n",
        "        data_root = dirpath\n",
        "        break\n",
        "print(f\"Bulunan DATA_ROOT (içinde train/test olan): {data_root}\")\n",
        "\n",
        "# 2. Tüm dosya yollarını 'glob' ile bulma\n",
        "DATA_ROOT = data_root # Bulunan yolu kullan\n",
        "train_dir = os.path.join(DATA_ROOT, \"train\")\n",
        "test_dir  = os.path.join(DATA_ROOT, \"test\")\n",
        "\n",
        "train_real = glob.glob(os.path.join(train_dir, \"REAL\", \"*\"))\n",
        "train_fake = glob.glob(os.path.join(train_dir, \"FAKE\", \"*\"))\n",
        "test_real  = glob.glob(os.path.join(test_dir, \"REAL\", \"*\"))\n",
        "test_fake  = glob.glob(os.path.join(test_dir, \"FAKE\", \"*\"))\n",
        "\n",
        "# 3. DataFrame'leri oluşturma (0=REAL, 1=FAKE)\n",
        "df_train = pd.DataFrame({\n",
        "    \"path\": train_real + train_fake,\n",
        "    \"label\": [0]*len(train_real) + [1]*len(train_fake)\n",
        "})\n",
        "df_test = pd.DataFrame({\n",
        "    \"path\": test_real + test_fake,\n",
        "    \"label\": [0]*len(test_real) + [1]*len(test_fake)\n",
        "})\n",
        "\n",
        "df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df_test  = df_test.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# 4. Veriyi K-Fold (20k) ve Kilitli Test (20k) olarak bölme\n",
        "egitim_alt_kume_df, _ = train_test_split(\n",
        "    df_train,\n",
        "    train_size=20000,\n",
        "    random_state=42,\n",
        "    stratify=df_train[\"label\"]\n",
        ")\n",
        "kilitli_test_df = df_test.copy()  # Orijinal test setini kilitliyoruz\n",
        "\n",
        "print(f\"K-Fold için alt küme boyutu: {len(egitim_alt_kume_df)}\")\n",
        "print(f\"Kilitli Test Seti boyutu: {len(kilitli_test_df)}\")\n",
        "print(f\"Alt küme dağılımı:\\n {egitim_alt_kume_df['label'].value_counts(normalize=True)}\")\n",
        "\n",
        "print(\"\\n--- Blok 2 Tamamlandı. Veri setleri (DataFrame) hafızada. ---\")"
      ],
      "metadata": {
        "id": "5hO2TlF7g2AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOK 3: ÖZELLİK ÇIKARIMI (ResNet50 @ 224x224) ---\n",
        "# (A100 GPU ile 5-10 dk sürecek)\n",
        "print(\"Blok 3 Çalışıyor: Özellik Çıkarımı (ResNet50 @ 224x224)...\")\n",
        "\n",
        "# 1. Cihazı (GPU) Ayarlama\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Kullanılan Cihaz (MUTLAKA 'cuda' OLMALI): {device}\")\n",
        "\n",
        "# 2. Görüntü Dönüşümleri (ResNet50 için 224x224)\n",
        "transform_resnet50 = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 3. Özel ImageDataset Sınıfı\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, transform):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.df.loc[idx, \"path\"]\n",
        "        label = int(self.df.loc[idx, \"label\"])\n",
        "        try:\n",
        "            img = Image.open(path).convert(\"RGB\")\n",
        "            img = self.transform(img)\n",
        "            return img, label\n",
        "        except Exception as e:\n",
        "            return torch.zeros((3, 224, 224)), -1 # Bozuksa -1 label'ı ver\n",
        "\n",
        "def collate_fn_safe(batch):\n",
        "    batch = list(filter(lambda x: x[1] != -1, batch)) # -1 label'lıları atla\n",
        "    if len(batch) == 0:\n",
        "        return torch.Tensor(), torch.Tensor()\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "# 4. ResNet50 Modelini Özellik Çıkarıcı Olarak Yükleme\n",
        "resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "resnet.fc = nn.Identity() # Son katmanı (sınıflandırıcıyı) kaldır\n",
        "resnet = resnet.to(device)\n",
        "resnet.eval()\n",
        "\n",
        "# 5. Özellik Çıkarıcı Fonksiyon\n",
        "@torch.no_grad()\n",
        "def extract_features(df, transform_to_use, batch_size=64):\n",
        "    dataset = ImageDataset(df, transform_to_use)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_safe)\n",
        "    feats, labels = [], []\n",
        "\n",
        "    for imgs, labs in tqdm(loader, desc=f\"Özellik Çıkarımı ({len(df)} görsel)\"):\n",
        "        if imgs.shape[0] == 0: continue\n",
        "        imgs = imgs.to(device)\n",
        "        f = resnet(imgs) # [B, 2048]\n",
        "        feats.append(f.cpu().numpy())\n",
        "        labels.append(labs.numpy())\n",
        "\n",
        "    return np.concatenate(feats, axis=0), np.concatenate(labels, axis=0)\n",
        "\n",
        "# 6. Fonksiyonu Çalıştırma (ResNet50 dönüşümü ile)\n",
        "print(\"K-Fold Eğitim Seti için özellik çıkarımı başlıyor...\")\n",
        "X_train_all, y_train_all = extract_features(egitim_alt_kume_df, transform_resnet50)\n",
        "\n",
        "print(\"Kilitli Test Seti için özellik çıkarımı başlıyor...\")\n",
        "X_test_locked, y_test_locked = extract_features(kilitli_test_df, transform_resnet50)\n",
        "\n",
        "print(f\"Train feature shape: {X_train_all.shape}, Train label shape: {y_train_all.shape}\")\n",
        "print(f\"Test feature shape: {X_test_locked.shape}, Test label shape: {y_test_locked.shape}\")\n",
        "print(\"\\n--- Blok 3 Tamamlandı. 'resnet', 'X_train_all' ve 'X_test_locked' hafızada. ---\")"
      ],
      "metadata": {
        "id": "s2Rbxk2Bg2Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOK 4: ŞAMPİYON MODELİ YENİDEN EĞİTME (HIZLI YOL) ---\n",
        "# (K-Fold DEĞİL, tek bir eğitim. ~15-20 dk sürecek)\n",
        "print(\"Blok 4 Çalışıyor: Şampiyon Model (SVM_RBF) Yeniden Eğitiliyor...\")\n",
        "\n",
        "# 1. Şampiyon modelimizi (SVM_RBF) tanımlıyoruz\n",
        "# (NOT: Eğer 'best_name' XGBoost çıktıysa, buradaki 'best_model'i ona göre değiştir)\n",
        "best_name = \"SVM_RBF\"\n",
        "best_model = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\", probability=True))\n",
        "\n",
        "print(f\"Şampiyon model '{best_name}' hafızaya yükleniyor...\")\n",
        "print(f\"'{best_name}' modeli TÜM (20,000) eğitim verisiyle yeniden eğitiliyor...\")\n",
        "print(\"(Bu işlem 15-20 dakika sürebilir, lütfen bekleyin...)\")\n",
        "\n",
        "start_train_time = time.time()\n",
        "# 2. Modeli TÜM X_train_all verisiyle EĞİT\n",
        "best_model.fit(X_train_all, y_train_all)\n",
        "end_train_time = time.time()\n",
        "\n",
        "print(f\"Eğitim Tamamlandı! (Süre: {(end_train_time - start_train_time)/60:.2f} dakika)\")\n",
        "print(\"\\n--- Blok 4 Tamamlandı. 'best_model' (Şampiyon) artık hafızada. ---\")"
      ],
      "metadata": {
        "id": "JC7w4wGVg2HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOK 5: DAYANIKLILIK (ROBUSTNESS) TESTİ İÇİN HAZIRLIK ---\n",
        "print(\"Blok 5 Çalışıyor: 'Bozukluk' (Distortion) Fonksiyonları Hazırlanıyor...\")\n",
        "\n",
        "# --- 1. Yeni 'Bozukluk' (Distortion) Transform'ları Tanımlama ---\n",
        "\n",
        "# A) Gaussian Gürültü (Karıncalanma) Ekleme\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n",
        "\n",
        "# B) JPEG Sıkıştırması (Instagram/Twitter Simülasyonu)\n",
        "class AddJpegCompression(object):\n",
        "    def __init__(self, quality=50):\n",
        "        self.quality = quality\n",
        "    def __call__(self, img_tensor):\n",
        "        img = T.ToPILImage()(img_tensor)\n",
        "        buffer = io.BytesIO()\n",
        "        img.save(buffer, \"JPEG\", quality=self.quality)\n",
        "        img_jpeg = Image.open(buffer)\n",
        "        return T.ToTensor()(img_jpeg)\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(quality={self.quality})'\n",
        "\n",
        "# --- 2. 'Bozukluk' İçin Yeni Dönüşüm Pipelineları ---\n",
        "transform_jpeg = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(), # Önce Tensöre çevir\n",
        "    AddJpegCompression(quality=50), # JPEG Sıkıştırması uygula (boz)\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize et\n",
        "])\n",
        "transform_noise = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # ÖNCE Normalize et\n",
        "    AddGaussianNoise(mean=0., std=0.1) # Sonra gürültü ekle\n",
        "])\n",
        "transform_blur = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.0)), # Bulanıklık ekle\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize et\n",
        "])\n",
        "\n",
        "# --- 3. Bozuk Veri İçin 'ImageDataset' Sınıfı ---\n",
        "class RobustnessImageDataset(Dataset):\n",
        "    def __init__(self, df, transform):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.df.loc[idx, \"path\"]\n",
        "        label = int(self.df.loc[idx, \"label\"])\n",
        "        try:\n",
        "            img = Image.open(path).convert(\"RGB\")\n",
        "            img = self.transform(img) # VERİLEN 'BOZUK' TRANSFORMU UYGULA\n",
        "            return img, label\n",
        "        except Exception as e:\n",
        "            return torch.zeros((3, 224, 224)), -1 # Bozuksa -1 label'ı ver\n",
        "\n",
        "print(\"\\n--- Blok 5 Tamamlandı. 'Bozuk' veri yükleyicileri hazır. ---\")"
      ],
      "metadata": {
        "id": "me_Hg2x2g2Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOK 6: DAYANIKLILIK (ROBUSTNESS) TESTİ ÇALIŞTIRMA VE FİNAL ---\n",
        "print(\"Blok 6 Çalışıyor: FİNAL DAYANIKLILIK TESTLERİ BAŞLIYOR...\")\n",
        "\n",
        "# 1. Test Fonksiyonu (Resimleri bozar, ResNet'ten geçirir, SVM'e sorar)\n",
        "@torch.no_grad() # Bu testte gradyan (eğitim) yok\n",
        "def test_robustness(ml_model, feature_extractor, df, transform_to_use):\n",
        "\n",
        "    print(f\"\\nTest başlıyor (Transform: {transform_to_use.__repr__()})...\")\n",
        "\n",
        "    # 1. Bozuk veriyi yükle\n",
        "    dataset = RobustnessImageDataset(df, transform_to_use)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False, collate_fn=collate_fn_safe)\n",
        "\n",
        "    feature_extractor.eval() # ResNet50'yi 'eval' moduna al\n",
        "\n",
        "    all_distorted_features = [] # Bozuk özellikleri topla\n",
        "    all_labels = []\n",
        "\n",
        "    # 2. ResNet50 (GPU) ile 'bozuk özellikleri' çıkar\n",
        "    for imgs, labs in tqdm(loader, desc=\"Bozuk Özellikler Çıkarılıyor\"):\n",
        "        if imgs.shape[0] == 0: continue\n",
        "        imgs = imgs.to(device) # Resimleri GPU'ya yolla\n",
        "        f = feature_extractor(imgs) # ResNet50'den özellikleri al\n",
        "        all_distorted_features.append(f.cpu().numpy()) # Özellikleri CPU'ya geri al\n",
        "        all_labels.append(labs.numpy())\n",
        "\n",
        "    X_distorted = np.concatenate(all_distorted_features)\n",
        "    y_true = np.concatenate(all_labels)\n",
        "\n",
        "    # 3. SVM/XGBoost (CPU) ile tahmin yap\n",
        "    print(f\"'{best_name}' modeli ile {len(X_distorted)} adet bozuk özellik üzerinde tahmin yapılıyor...\")\n",
        "    y_pred = ml_model.predict(X_distorted)\n",
        "\n",
        "    # 4. Metrikleri hesapla\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
        "\n",
        "    print(f\"Test Bitti: F1-Skoru = {f1*100:.2f}%\")\n",
        "    return acc, prec, rec, f1\n",
        "\n",
        "# 2. Testleri Çalıştırma\n",
        "robust_results = {}\n",
        "try:\n",
        "    # Test 1: JPEG Sıkıştırma\n",
        "    acc_jpeg, pre_jpeg, rec_jpeg, f1_jpeg = test_robustness(\n",
        "        best_model, resnet, kilitli_test_df, transform_jpeg\n",
        "    )\n",
        "    robust_results[\"Sıkıştırılmış JPEG (%50)\"] = f1_jpeg\n",
        "\n",
        "    # Test 2: Gürültü Ekleme\n",
        "    acc_noise, pre_noise, rec_noise, f1_noise = test_robustness(\n",
        "        best_model, resnet, kilitli_test_df, transform_noise\n",
        "    )\n",
        "    robust_results[\"Gaussian Gürültü (std=0.1)\"] = f1_noise\n",
        "\n",
        "    # Test 3: Bulanıklık Ekleme\n",
        "    acc_blur, pre_blur, rec_blur, f1_blur = test_robustness(\n",
        "        best_model, resnet, kilitli_test_df, transform_blur\n",
        "    )\n",
        "    robust_results[\"Gaussian Bulanıklık\"] = f1_blur\n",
        "\n",
        "    # 3. FİNAL RAPORUNU OLUŞTURMA\n",
        "    print(\"\\n\\n===== HİBRİT MODEL DAYANIKLILIK (ROBUSTNESS) ANALİZİ =====\")\n",
        "    print(f\"Test Edilen Şampiyon Model: {best_name} (Hibrit ML)\\n\")\n",
        "\n",
        "    # Temiz veri sonucunu da hesaplayalım (ne olur ne olmaz)\n",
        "    print(\"Temiz veri skoru yeniden hesaplanıyor (karşılaştırma için)...\")\n",
        "    y_pred_clean = best_model.predict(X_test_locked)\n",
        "    clean_f1 = f1_score(y_test_locked, y_pred_clean, average=\"binary\")\n",
        "\n",
        "    print(f\"Karşılaştırma (F1-Skoru %):\")\n",
        "    print(\"----------------------------------------------------------\")\n",
        "    print(f\"  Temiz Veri Seti (Kilitli Test): \\t{clean_f1*100:.2f}%\")\n",
        "    print(\"----------------------------------------------------------\")\n",
        "    for test_name, f1_score in robust_results.items():\n",
        "        print(f\"  {test_name}: \\t{f1_score*100:.2f}%\")\n",
        "    print(\"----------------------------------------------------------\")\n",
        "\n",
        "    # Bu sonuçları da bir dosyaya kaydet\n",
        "    with open(\"Hibrit_Dayaniklilik_Raporu.txt\", \"w\") as f:\n",
        "        f.write(f\"===== HİBRİT MODEL DAYANIKLILIK (ROBUSTNESS) ANALİZİ =====\\n\")\n",
        "        f.write(f\"Test Edilen Model: {best_name} (Hibrit ML)\\n\\n\")\n",
        "        f.write(f\"Karşılaştırma (F1-Skoru %):\\n\")\n",
        "        f.write(\"----------------------------------------------------------\\n\")\n",
        "        f.write(f\"  Temiz Veri Seti (Kilitli Test): \\t{clean_f1*100:.2f}%\\n\")\n",
        "        f.write(\"----------------------------------------------------------\\n\")\n",
        "        for test_name, f1_score in robust_results.items():\n",
        "            f.write(f\"  {test_name}: \\t{f1_score*100:.2f}%\\n\")\n",
        "        f.write(\"----------------------------------------------------------\\n\")\n",
        "    print(\"\\n'Hibrit_Dayaniklilik_Raporu.txt' dosyası da kaydedildi.\")\n",
        "\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"\\n--- HATA --- ({e})\")\n",
        "    print(\"Görünüşe göre Blok 3 veya 4'te bir sorun oldu ve 'best_model' veya 'resnet' hafızaya yüklenemedi.\")\n",
        "    print(\"Lütfen Blok 1'den itibaren tekrar çalıştırmayı deneyin.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nBaşka bir hata oluştu: {e}\")\n",
        "\n",
        "print(\"\\n--- Blok 6 Tamamlandı. Özgünlük (Dayanıklılık) analizi bitti. ---\")"
      ],
      "metadata": {
        "id": "JLhIq4tNg2L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3kJ5PJ0sg2OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XkmFC6ILg2Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "na7Day5_g2TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Relu0NZ6g2Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "meErYaXmg2X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qwExMjJgg2aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WqGEV2ACg2dK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}